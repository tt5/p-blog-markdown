# Introduction

matrix $A\in \mathbb{F}^{m\times n}$

$A_{m\times n}$ produces a linear transformation from $R^n$ to  $R^m$, the
action of $A$ on the four subspaces of $R^n$ and $R^m$ with the right bases.
Those bases come from the singular value decomposition (SVD).

$Ax = b$

singular $A_{n\times n}$, $Ax = 0$, $x \neq 0$

null space $\text{N}(A) = \{ x \in \mathbb{F}^n : Ax = 0 \}$

column space $\text{R}(A) = \{ y \in \mathbb{F}^m : Ax = y \text{for
some} x \in \mathbb{F}^n \}$

row space $\text{R}(A^T) = \{ y \in \mathbb{F}^n : A^Tx=y \text{ for
some } x \in \mathbb{F}^m \}$

$\text{dim}\,R(A) = \text{dim}\,R(A^T)$

$\text{dim}\,R(A) + \text{dim}\,N(A)=n$

The nullspace $N(A)$ is orthogonal to the row space $R(A^T)$

null space of $A^T$, $N(A^T)$  
$A^Ty=0$  
$y^TA=0^T$

$P$ Permutation. The Identity matrix with reordered rows.

If $S$ is a subspace of a vector space $\mathbb{F}^n$ the **orthogonal
complement** of $S$ is defined as $$S^\bot \{ x \in \mathbb{F}^n : x^T
y = 0 \text{ for every } y \in S\}$$

Theorem:
For a matrix $A$ whe have $N(A) = R(A^T)^\bot$ and $N(A^T)=R(A)^\bot$.

If $V$ is a subspace of $\mathbb{F}^n$, then $\text{dim}(V) +
\text{dim}(V^\bot)=n$.

If $V,W$ are subspaces then $\text{dim}(V) + \text{dim}(W) =
\text{dim}(V \cap W) + \text{dim}(V + W)$ where $V+W = \{ v+w : v\in
V, w\in W\}$

**rank**, $r = \text{dim}(C(A))=\text{dim}(\text{rowspan}(A))$

The action of $A_{m\times n}$:
($m$ rows (down index), $n$ columns (right index)), $A=[a_1,\dots,a_n]$
row space -> column space  
null space -> $0$  

columns space = span of its column vectors, $C(A)$ or $R(A)$ "Range",
$\text{colspace}(A) = \text{span}\{a_1,\dots, a_n\}$

$A_{m\times n}$, linear transformation, columnspace = Image.

$A(i,j)$  
$i$ rows, $j$ columns

$(a_{ij})_{m×n}$ means that $A$ is $m\times n$ matrix with $A(i,j)=a_{ij}$

SVD:

The basis vectors are orthogonal. The matrix $\Sigma$ with respect to
these bases is diagonal

$$
\Sigma =
\begin{pmatrix} \sigma_1 \text{ or } 0 && ²nn¹ & \ddots & ²nn¹ && \sigma_r \text{ or } 0 \end{pmatrix}
$$

$v_1, \dots, v_n$ basis for the row space  
$u_1, \dots, v_r$ basis for the column space

$Av_j = \sigma_i u_i$, $\sigma_i > 0$

$A^TAv_i = \sigma_i^2 v_i$

$AV = U\Sigma$  
$A = U\Sigma V^T$

$\displaystyle
A = U_{m\times m} \Sigma_{m\times n} V_{n\times n}^T
$

$U$ orthogonal, its columns $u_1, \dots, u_r, \dots, u_m$ are basis
vectors for the columns space and the left nullspace.

$\Sigma$, diagonal

$V^T$, orthogonal, columns of $V$, $v_1, \dots, v_r, \dots, v_n$ are
basis vectors for the row space and nullspace.

$\bar{A}$ complex conjugate entrywise  
$A^\star=(\bar{A})^{T}$

$AXA=A$\quad |$A^{-1}$ left and right  
$X=A^{-1}$

nonsingular: columns (or rows) are linearly independent.

Theorem 1:  
$GL_n(\mathbb{F})$ is generated by elementary matrices.
With relations:  
$x_\alpha(f_1) x_\beta(f_2) = x_\beta(f_2) x_\alpha(f_1) \cdot$ `(some explicit stuff)`  
for prenilpotent pair of elementary matrices.

Theorem 2:  
LDU decomposition

# ---

A symmetric bilinear form $⟨\cdot,\cdot ⟩$ is **positive definite**
if and only if it is the Euclidean inner product with respect to some bases.

$V$ Vectorspace,

a **bilinear form** is a function
$B : V × V → K$ which is linear in each argument separately:

$B(u + v, w) = B(u, w) + B(v, w)$,
$B(u, v + w) = B(u, v) + B(u, w)$,
$B(λu, v) = B(u, λv) = λB(u, v)$.

A **symplectic bilinear form** is

- a bilinear form
- alternating: $w(v,v)=0$ holds for all $v\in V$
- nondegenerate

$V$ finite dimensional vector space

A **nondegenerate bilinear form** $B$ is one for which the
associated matrix is non-singular. $B$ is nondegenerate if
$B(v,w)=0$ for all $w$ implies $v=0$.

Given any linear map $A: v\to V\star$ one can optain
a bilinear form $B$ on $V$ via $B(v)(w)$.
This form will be nondegenerate if and only if
$A$ is an isomorphism.

## Matrix

A **minor** of a matrix $A$ is the determinant of some
smaller square matrix, cut down from $A$ by removing one or more
of its rows or columns.

**Projection** $P$. $P=A(A^TA)^{-1}A^T$.
$P$ symmetric. $PP=P$.

For every Matrix the column rank is equal
to the row rank.

$A_{n\times n}$,
$Q$ unitary,
$U$ upper triangular,
$A=QUQ^\star$

$A_{n\times n}$ Matrix. Equivalent:

1)\q  $A$ is invertible  
2)\q  $\operatorname{rank}(A)=n$  
3)\q  $\det(A)\ne 0$  
4)\q  $\lambda_{i}\ne 0$, for all $i$ (Eigenvalues)  

Eine $n\times n$ Matrix heisst **digonalisierbar**
falls sie *similar* ist zu einer Diagonalmatrix.

Any symmetric matrix can be diagonalized.

Let A = A T be a real symmetric n × n matrix. Then  
(a)\q All the eigenvalues of A are real.  
(b)\q Eigenvectors corresponding to distinct eigenvalues are orthogonal.  
(c)\q There is an orthonormal basis of R n consisting of n eigenvectors of A.  

The most important subclass of symmetric matrices are the positive definite matrices.

The term **spectrum** refers to the eigenvalues of a matrix.

The set (multi-set) of all the eigenvalues of a square complex matrix $A$ is called
the *spectrum* of $A$, and is denoted by $\sigma(A)$.

**Spectral radius**: the maximum absolute value of any of the eigenvalues.
$\rho(A) = \text(max)\lbrace |\lambda| : \lambda \in \sigma(A)\rbrace$

A **defective** matrix $n\times n$ does not have
$n$ linearly independent eigenvectors.

$A$ is **normal** iff there exists a diagonal matrix $\Lambda$
and a unitary matrix $U$ such that $A=U\Lambda U^\star$.

$A$ is normal if $A^\star A=AA^\star$.

Positive definite matrices are closely related to positive-definite symmetric
bilinear forms (or sesquilinear forms in the complex case), and to inner
products of vector spaces.

$A$ and $B$ both $n\times n$ matrices are **similar** means for some $M$,
$B=M^{-1}AM$. They have the same eigenvalues.  One can treat similar matrices
as different matrix representation of the same linear operator
(transformation).

$Q$ orthogonal,$R$ upper triangular,
$A=QR$. If $A$ has $n$ linearly independent
columns, then the first $n$ columns of $Q$
form an orthonormal basis of the column space of $A$.

The Gram–Schmidt procedure for orthonormalizing bases of R n can be
reinterpreted as a matrix factorization.

$A_{n\times n}$ real matrix with $n$ linearly independent
eigenvectors. $A=Q\Lambda Q^{-1}$, $Q$ $n\times n$ matrix, columns
are the eigenvectors of $A$, $\Lambda$ diagonal, $\Lambda_{ii}=\lambda_i$

$A_{n\times n}$ invertible, $u,v$ column vectors.
$$\operatorname{det}(A+uv^T)=(1+v^TA^{-1}u)\operatorname{det}(A)$$
$$(A+uv^T)^{-1}=A^{-1}\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}$$

A Hankel matrix is an upside-down Toeplitz matrix.

A Toeplitz matrix or diagonal-constant matrix is a matrix in which each
descending diagonal from left to right is constant.

A Hurwitz matrix, or Routh-Hurwitz matrix, in engineering stability matrix, is a
structured real square matrix constructed with coefficients of a real polynomial.

It was established by Adolf Hurwitz in 1895 that a real polynomial
is stable (that is, all its roots have strictly negative real part)
if and only if all the leading principal minors of the matrix $H(p)$ are positive:

$\mathbf{A}$ is sure to have $n$ independent eigenvectors if all eigenvalues be
destinct.

Let $A$ be a *real, symmetric* $n\times n$ Matrix.
Then there esists an orthogonal matrix $Q$ such
that $A=Q\Lambda Q^T$, where $\Lambda$ is a real
diagonal matrix. The eigenvalues of $A$ appear on the diagonal of
$\Lambda$, while the columns of $Q$ are the corresponding
orthonormal eigenvectors.

A $n\times n$ matrix $A$ is called **positive definite** if
it satisfies the positivity condition $x^TAx>0$ for all
$0\ne x\in\mathbb{R}^n$.

A Hermetian $n\times n$ matrix $A$ is positive definite if
$x\star Mx > 0$ for every $x\neq 0$.

A symmetric matrix is positive definite iff all of its
eigenvalues are strictly positive.

A symmetric matrix is positive definite iff all its
principal minors are $> 0$.

The kernel of a matrix, also called the null space, is the
kernel of the linear map defined by the matrix.

Ein K-Vektorraum, der mit einem Skalarprodukt versehen ist,
heisst **euklidisch**, falls $\mathbf{K}=\mathbf{R}$ und **unitär**,
falls $\mathbf{K}=\mathbf{C}$.

**Vectorraum**. Eine Menge von Vektoren, closed
under linear combination (with real scalar).

$A_{m\times n}$ real matrix. $A=U\Sigma V^T$.
rank $r$.

Rowspace ($R^n$), the first $r$ columns of $V$,
and nullspace ($R^n$), the last $(n-r)$ columns of $V$,
are orthogonal complement.
Columnspace ($R^m$), the first $r$ columns
of $U$, and the nullspace of
$A^T$ ($R^m$), the last $(m-r)$ columns
of $U$, are orthogonal complement.

columnspace = all linear combinations of the columns.

nullspace of $A^T$ = left nullspace.

nullpace: how to combine columns to get 0.

LDU decomposition: $A=LDU$, $L,U$ unit triangular, $D$ diagonal.

**unit triangular**:

$$
\begin{pmatrix} 1 &  & 0 ²nn¹ & \ldots &  ²nn¹ \star & & 1 \end{pmatrix}
$$

or

$$
\begin{pmatrix} 1 &  & \star ²nn¹ & \ldots & ²nn¹ 0 & & 1 \end{pmatrix}
$$

$M_{m\times n}$ real or complex.

$M=U\Sigma V^\star$.
$U_{m\times m}$ unitary, left-singular vectors (columns).
$\Sigma_{m\times n}$ rectangular diagonal with
non-negative real numbers, **singular values**.
$V^\star_{n\times n}$ unitary, right-singular vectors (columns).

The left-singular (right-singular) vectors of $M$
are eigenvalues of $MM^\star $ (respectivelly $M^\star M$).

The non-zero singular values of $M$ are the square roots
of the non-zero eigenvalues of both $M^\star M$ and $MM^\star $.

\minisec{linear Transformation}

**Linear map**. Homomorphism of vector spaces.

$T(v+w)=T(v)+T(w)\quad T(cv)=cT(v)$

or

$T(cv+dw)=cT(v)+dT(w)$

$T(v)=\mathbf{A}\mathbf{v}$

If $A* = -A$, then $A$ is called **skew-Hermitian**.

If $A*A = I$, then $A$ is called **unitary**.

The Cholesky decomposition of a Hermitian positive-definite matrix A is a
decomposition of the form $\mathbf{A = L L}^{\star }$ where $L$ is a lower
triangular matrix with real and positive diagonal entries

Every Hermitian positive-definite matrix (and thus also every real-valued
symmetric positive-definite matrix) has a unique Cholesky decomposition.

The Cholesky decomposition is mainly used for the numerical solution of linear
equations $Ax = b$. If $A$ is symmetric and positive definite, then
we can solve $Ax = b$ by first computing the Cholesky decomposition
$A = LL^\star $, then solving $Ly = b$ for $y$ by forward
substitution, and finally solving $L^\star x = y$ for $x$ by back substitution.

In mathematics, a P-matrix is a complex square matrix with every principal
minor > 0. A closely related class is that of $P_{0}$-matrices, which are the
closure of the class of P-matrices, with every principal minor ≥  $\geq 0$.
The class of nonsingular M-matrices is a subset of the class of P-matrices.
More precisely, all matrices that are both P-matrices and Z-matrices are
nonsingular M-matrices.

A matrix $X ∈ R^{n,n}$ is a Z-matrix if $X = qI − P$ , where P ≥ 0 and q ∈ R.
If, in addition, q ≥ ρ(P ), where ρ(P ) is the spectral radius of P , then X is
an M-matrix, and is singular if and only if $q = ρ(P )$.

An M-matrix is a Z-matrix with eigenvalues whose real parts are positive.
M-matrices are also a subset of the class of P-matrices, and also of the class
of inverse-positive matrices (i.e. matrices with inverses belonging to the
class of positive matrices).

In mathematics, the class of Z-matrices are those matrices whose off-diagonal
entries are less than or equal to zero

Let A be a n × n real Z-matrix, then the following statements are equivalent to
A being a non-singular M-matrix:

- ²q¹ All the principal minors of A are positive.  
- ²q¹ Every real eigenvalue of A is positive.  
- ²q¹ All the leading principal minors of A are positive.  
- ²q¹ There exist lower and upper triangular matrices L and U respectively, with positive diagonals, such that A = LU.  
- ²q¹ A is positive stable. That is, the real part of each eigenvalue of A is positive.  
- ²q¹ A is semi-positive. That is, there exists $x>0$ with $Ax>0$  
- ²q¹ There exists x ≥ 0 with $Ax>0$.  

\minisec{Smith Normal Form}

The **Smith normal form** of a matrix is diagonal, and can be obtained from the
original matrix by multiplying on the left and right by invertible square
matrices. $SAT$.
The matrices S and T can be found by starting out with identity matrices of the
appropriate size, and modifying S each time a row operation is performed on A
in the algorithm by the same row operation, and similarly modifying T for each
column operation performed.
Only invertible row and column operations are performed, which ensures that S
and T remain invertible matrices.

For a in $R \ {0}$, write $δ(a)$ for the number of prime factors of $a$.

As an example, we will find the Smith normal form of the following matrix over
the integers.

$$
\begin{pmatrix}2&4&4 ²nn¹ -6&6&12 ²nn¹ 10&-4&-16\end{pmatrix}
$$

Step I: Choosing a pivot

Choose jt to be the smallest column index of A with a non-zero entry, starting
the search at column index jt-1+1 if t > 1.

We wish to have $a_{t,j_{t}}\neq 0$ ; if this is the case this step is
complete, otherwise there is by assumption some k with $a_{k,j_{t}}\neq 0$ ,
and we can exchange rows $t$ and k, thereby obtaining $a_{t,j_{t}}\neq 0$. 

Our chosen pivot is now at position $(t, j_t)$.

$$
\to {\begin{pmatrix}2&0&0 ²nn¹ -6&18&24 ²nn¹ 10&-24&-36\end{pmatrix}}
$$

Step II: Improving the pivot

If there is an entry at position (k,jt) such that
$a_{t,j_{t}}\nmid a_{k,j_{t}}$,
then, letting $\beta =\gcd \left(a_{t,j_{t}},a_{k,j_{t}}\right)$, we know by
the Bézout property that
there exist σ, τ in R such that

$$
a_{t,j_{t}}\cdot \sigma +a_{k,j_{t}}\cdot \tau =\beta .
$$

By left-multiplication with an appropriate invertible matrix L, it can be
achieved that row t of the matrix product is the sum of σ times the original
row t and τ times the original row k, that row k of the product is another
linear combination of those original rows, and that all other rows are
unchanged. Explicitly, if σ and τ satisfy the above equation, then for α = a t
$\alpha =a_{t,j_{t}}/\beta $ $\gamma =a_{k,j_{t}}/\beta $ (which divisions are possible
by the definition of β) one has

$$
\sigma \cdot \alpha +\tau \cdot \gamma =1,
$$

so that the matrix

$$
L_{0}={\begin{pmatrix}\sigma &\tau  ²nn¹ -\gamma &\alpha  ²nn¹ \end{pmatrix}}
$$

is invertible, with inverse

$$
{\begin{pmatrix}\alpha &-\tau  ²nn¹ \gamma &\sigma  ²nn¹ \end{pmatrix}}.
$$

Now L can be obtained by fitting $L_{0}$ into rows and
columns t and k of the identity matrix. By construction the matrix obtained
after left-multiplying by L has entry β at position $(t,j_t)$ (and due to our
choice of α and γ it also has an entry 0 at position $(k,j_t)$, which is useful
though not essential for the algorithm). This new entry β divides the entry
$a_{t,j_{t}}$ that was there before, and so
in particular $\delta (\beta )<\delta (a_{t,j_{t}})$ therefore repeating
these steps must eventually terminate. One ends up with a matrix having an
entry at position $(t,j_t)$ that divides all entries in column $j_t$.

$$
\to {\begin{pmatrix}2&0&0 ²nn¹ 0&18&24 ²nn¹ 0&-24&-36\end{pmatrix}}
$$

Step III: Eliminating entries

Finally, adding appropriate multiples of row t, it can be achieved that all
entries in column jt except for that at position (t,jt) are zero. This can be
achieved by left-multiplication with an appropriate matrix. However, to make
the matrix fully diagonal we need to eliminate nonzero entries on the row of
position (t,jt) as well. This can be achieved by repeating the steps in Step II
for columns instead of rows, and using multiplication on the right by the
transpose of the obtained matrix L. In general this will result in the zero
entries from the prior application of Step III becoming nonzero again.

However, notice that the ideals generated by the elements at position (t,jt)
form an ascending chain, because entries from a later step always divide
entries from a previous step. Therefore, since R is a Noetherian ring (it is a
PID), the ideals eventually become stationary and do not change. This means
that at some stage after Step II has been applied, the entry at (t,jt) will
divide all nonzero row or column entries before applying any more steps in Step
II. Then we can eliminate entries in the row or column with nonzero entries
while preserving the zeros in the already-zero row or column. At this point,
only the block of A to the lower right of (t,jt) needs to be diagonalized, and
conceptually the algorithm can be applied recursively, treating this block as a
separate matrix. In other words, we can increment t by one and go back to Step
I.

$$
\to {\begin{pmatrix}2&0&0 ²nn¹ 0&18&24 ²nn¹ 0&-6&-12\end{pmatrix}}
$$

Final step

Applying the steps described above to the remaining non-zero columns of the
resulting matrix (if any), we get an $m\times n$-matrix with column indices
$j_{1}<\ldots <j_{r}$ where $r\leq \min(m,n)$. The matrix entries $(l,j_{l})$
are non-zero, and every other entry is zero.

Now we can move the null columns of this matrix to the right, so that the
nonzero entries are on positions $(i,i)$ for
$1\leq i\leq r$. For short, set
$\alpha _{i}$ for the element at position
$(i,i)$.

The condition of divisibility of diagonal entries might not be satisfied. For
any index $i<r$ for which
$\alpha _{i}\nmid \alpha _{i+1}$ one can
repair this shortcoming by operations on rows and columns $i$
and $i+1$ only: first add column
$i+1$ to column $i$ to get an entry
$\alpha _{i+1}$ in column i without disturbing the entry
$\alpha _{i}$ at position
$(i,i)$, and then apply a row operation to make the entry at position
$(i,i)$ equal to
$\beta =\gcd(\alpha _{i},\alpha _{i+1})$
as in Step II; finally proceed as in Step III to make the
matrix diagonal again. Since the new entry at position
$(i+1,i+1)$ is a linear combination of the original
$\alpha _{i},\alpha _{i+1}$,
it is divisible by β.

The value $\delta (\alpha _{1})+\cdots +\delta (\alpha _{r})$
does
not change by the above operation (it is δ of the determinant of the upper
$r\times r$ submatrix), whence that operation does
diminish (by moving prime factors to the right) the value of

$$
\sum _{j=1}^{r}(r-j)\delta
(\alpha _{j}).
$$

So after finitely many applications of this operation no further application is
possible, which means that we have obtained
$\alpha _{1}\mid \alpha _{2}\mid \cdots \mid \alpha _{r}$
as desired.

Since all row and column manipulations involved in the process are invertible,
this shows that there exist invertible $m\times m$ 
and $n\times n$ n-matrices S, T so that the
product S A T satisfies the definition of a Smith normal form. In particular,
this shows that the Smith normal form exists, which was assumed without proof
in the definition.

$$
\to {\begin{pmatrix}2&0&0 ²nn¹ 0&6&12 ²nn¹ 0&18&24\end{pmatrix}}
$$

$$
\to {\begin{pmatrix}2&0&0 ²nn¹ 0&6&12 ²nn¹ 0&0&-12\end{pmatrix}}
$$

$$
\to {\begin{pmatrix}2&0&0 ²nn¹ 0&6&0 ²nn¹ 0&0&12\end{pmatrix}}
$$

So the Smith normal form is

$$
{\begin{pmatrix}2&0&0 ²nn¹ 0&6&0 ²nn¹ 0&0&12\end{pmatrix}}
$$

and the invariant factors are 2, 6 and 12.

The Smith normal form can be used to determine whether or not matrices with
entries over a common field are similar. Specifically two matrices A and B are
similar if and only if the characteristic matrices $xI-A$
and $xI-B$ have the same Smith normal form.

For example, with

`A {}={`

$
\begin{bmatrix}1&2 ²nn¹ 0&1\end{bmatrix}
$
`}, {\mbox{SNF}}(xI-A)={`

$
\begin{bmatrix}1&0 ²nn¹ 0&(x-1)^{2}\end{bmatrix}
$

`}  ²nn¹  B&{}={`

$
\begin{bmatrix}3&-4 ²nn¹ 1&-1\end{bmatrix}
$

},

`{\mbox{SNF}}(xI-B)={`

$
\begin{bmatrix}1&0 ²nn¹ 0&(x-1)^{2}\end{bmatrix}
$
} 

`C&{}={`

$
\begin{bmatrix}1&0 ²nn¹ 1&2\end{bmatrix}
$

`}, {\mbox{SNF}}(xI-C)= {`
 
$
\begin{bmatrix}1&0 ²nn¹ 0&(x-1)(x-2)\end{bmatrix}
$

`}.\}`

A and B are similar because the Smith normal form of their characteristic
matrices match, but are not similar to C because the Smith normal form of the
characteristic matrices do not match.

### Jordan Normal Form

A **Jordan normal form** (often called Jordan canonical form) of a linear
operator on a finite-dimensional vector space is an upper triangular matrix of
a particular form called a Jordan matrix, representing the operator with
respect to some basis.
Such a matrix has each non-zero off-diagonal entry equal to 1, immediately
above the main diagonal (on the superdiagonal), and with identical diagonal
entries to the left and below them.

If the vector space is over a field K, then a basis with respect to which the
matrix has the required form exists if and only if all eigenvalues of the
matrix lie in K, or equivalently if the characteristic polynomial of the
operator splits into linear factors over K. This condition is always satisfied
if K is algebraically closed (for instance, if it is the field of complex
numbers). The diagonal entries of the normal form are the eigenvalues (of the
operator), and the number of times each eigenvalue occurs is called the
algebraic multiplicity of the eigenvalue.

If the operator is originally given by a square matrix M, then its Jordan
normal form is also called the Jordan normal form of M. Any square matrix has a
Jordan normal form if the field of coefficients is extended to one containing
all the eigenvalues of the matrix. In spite of its name, the normal form for a
given M is not entirely unique, as it is a block diagonal matrix formed of
Jordan blocks, the order of which is not fixed; it is conventional to group
blocks for the same eigenvalue together, but no ordering is imposed among the
eigenvalues, nor among the blocks for a given eigenvalue, although the latter
could for instance be ordered by weakly decreasing size.

The Jordan–Chevalley decomposition is particularly simple with respect to a
basis for which the operator takes its Jordan normal form. The diagonal form
for diagonalizable matrices, for instance normal matrices, is a special case of
the Jordan normal form

For complex matrices: It can be shown that the Jordan normal form of a given
matrix A is unique up to
the order of the Jordan blocks.

Example:

$$
A = \begin{bmatrix} 5 & 4 & 2 & 1 ²nn¹ 0 & 1 & -1 & -1 ²nn¹ -1 & -1 & 3 & 0 ²nn¹ 1 & 1 & -1 & 2 \end{bmatrix}
$$

The characteristic polynomial of A is

$$
\chi (\lambda ) = \det(\lambda I-A) = \lambda ^{4}-11\lambda ^{3}+42\lambda
^{2}-64\lambda +32 = (\lambda -1)(\lambda -2)(\lambda -4)^{2}.\,
$$

This shows that the eigenvalues are 1, 2, 4 and 4, according to algebraic
multiplicity. The eigenspace corresponding to the eigenvalue 1 can be found by
solving the equation $Av = λ v$. It is spanned by the column vector $v = (−1, 1,
0, 0)^T$. Similarly, the eigenspace corresponding to the eigenvalue 2 is spanned
by $w = (1, −1, 0, 1)^T$. Finally, the eigenspace corresponding to the eigenvalue
4 is also one-dimensional (even though this is a double eigenvalue) and is
spanned by $x = (1, 0, −1, 1)^T$. So, the geometric multiplicity (i.e., the
dimension of the eigenspace of the given eigenvalue) of each of the three
eigenvalues is one. Therefore, the two eigenvalues equal to 4 correspond to a
single Jordan block, and the Jordan normal form of the matrix A is the direct
sum

$$
J=J_{1}(1)\oplus J_{1}(2)\oplus
J_{2}(4)={\begin{bmatrix}1&0&0&0 ²nn¹ 0&2&0&0 ²nn¹ 0&0&4&1 ²nn¹ 0&0&0&4\end{bmatrix}}.
$$

There are three chains. Two have length one: {v} and {w}, corresponding to the
eigenvalues 1 and 2, respectively. There is one chain of length two
corresponding to the eigenvalue 4. To find this chain, calculate

m4 m4exit
$
\displaystyle
\ker {(A-4I)}^{2}=\operatorname {span}
\,\left\{{
\begin{bmatrix} 1 ²nn¹ 0 ²nn¹ 0 ²nn¹ 0\end{bmatrix}
},{
\begin{bmatrix}1 ²nn¹ 0 ²nn¹ -1 ²nn¹ 1\end{bmatrix}
}\right\}
$

where I is the 4 x 4 identity matrix. Pick a vector in the above span that is
not in the kernel of $A − 4I$, e.g., $y = (1,0,0,0)^T$. Now, $(A − 4I)y = x$ and
$(A − 4I)x = 0$, so ${y, x}$ is a
chain of length two corresponding to the eigenvalue 4.

The transition matrix P such that $P^{−1}AP = J$ is formed by putting these vectors
next to each other as follows

$$
P={\Big [}\,v\,{\Big |}\,w\,{\Big |}\,x\,{\Big |}\,y\,{\Big
]}={\begin{bmatrix}-1&1&1&1 ²nn¹ 1&-1&0&0 ²nn¹ 0&0&-1&0 ²nn¹ 0&1&1&0\end{bmatrix}}.
$$

A computation shows that the equation $P^{−1}AP = J$ indeed holds.

$$
P^{-1}AP=J={\begin{bmatrix}1&0&0&0 ²nn¹ 0&2&0&0 ²nn¹ 0&0&4&1 ²nn¹ 0&0&0&4\end{bmatrix}}.
$$

If we had interchanged the order of which the chain vectors appeared, that is,
changing the order of $v, w$ and ${x, y}$ together, the Jordan blocks would be
interchanged. However, the Jordan forms are equivalent Jordan forms.

\minisec{Jordan-Chevalley decomposition}

The multiplicative decomposition expresses an invertible operator as the
product of its commuting semisimple and unipotent parts.

Now, let $x$ be any operator. A Jordan–Chevalley decomposition of $x$ is an
expression of it as a sum:

$x = x_{ss} + x_n$,

where $x_{ss}$ is semisimple, $x_n$ is nilpotent, and $x_{ss}$ and $x_n$ commute. If
such a
decomposition exists it is unique, and $x_{ss}$ and xn are in fact expressible as
polynomials in $x$.

If $x$ is an invertible operator, then a multiplicative Jordan–Chevalley
decomposition expresses $x$ as a product:

$x = x_{ss} · x_u$,

where $x_{ss}$ is semisimple, $x_u$ is unipotent, and $x_{ss}$ and $x_u$ commute.
Again, if
such a decomposition exists it is unique, and $x_{ss}$ and $x_u$ are
expressible as
polynomials in $x$.

If $x$ is in the Jordan normal form, then $x_{ss}$ is the endomorphism whose
matrix on the same basis contains just the diagonal terms of $x$, and $x_n$ is
the endomorphism whose matrix on that basis contains just the off-diagonal
terms; $x_u$ is the endomorphism whose matrix is obtained from the Jordan
normal form by dividing all entries of each Jordan block by its diagonal
element.

If the ground field is not perfect, then a Jordan–Chevalley decomposition may
not exist.

In algebra, a field k is said to be perfect if any one of the following
equivalent conditions holds:

-\q Every irreducible polynomial over k has distinct roots.  
-\q Every irreducible polynomial over k is separable.  
-\q Every finite extension of k is separable.  
-\q Every algebraic extension of k is separable.  
-\q Either k has characteristic 0, or, when k has characteristic p > 0, every element of k is a pth power.  
-\q Either k has characteristic 0, or, when k has characteristic p > 0, the Frobenius endomorphism x→xp is an automorphism of k  
-\q The separable closure of k is algebraically closed.  
-\q Every reduced commutative k-algebra A is a separable algebra; i.e., $A\otimes _{k}F$ is reduced for every field extension F/k. (see below)  

Otherwise, k is called imperfect.

In particular, all fields of characteristic zero and all finite fields are perfect.

Perfect fields are significant because Galois theory over these fields becomes
simpler, since the general Galois assumption of field extensions being
separable is automatically satisfied over these fields (see third condition
above).

Another important property of perfect fields is that they admit Witt vectors.

A Witt vector is an infinite sequence of elements of a commutative ring. Ernst
Witt showed how to put a ring structure on the set of Witt vectors, in such a
way that the ring of Witt vectors over the finite field of order p is the ring
of p-adic integers.

# Some

The **permanent** of an n-by-n matrix $A = (a_{i,j})$ is defined as

$$
\operatorname {perm} (A)=\sum _{\sigma \in S_{n}}
\prod _{i=1}^{n}a_{i,\sigma (i)}
$$

The sum here extends over all elements σ of the symmetric group $S_n$; i.e.
over all permutations of the numbers 1, 2, ..., n.


$$
\mathrm{perm} (A)=(-1)^{n}\sum _{S\subseteq \{1,\dots
,n\}}(-1)^{|S|}\prod _{i=1}^{n}\sum _{j\in S}a_{ij}.
$$

The definition of the permanent of A differs from that of the determinant of A
in that the signatures of the permutations are not taken into account.

The permanent of a (0,1)-matrix is equal to the number of vertex cycle covers
of an unweighted directed graph.

For an unweighted bipartite graph, if we set $a_{i,j} = 1$ if there is an edge
between the vertices $x_{i}$ and $y_{j}$ and $a_{i,j} = 0$ otherwise, then each
perfect matching has weight 1.  Thus the number of perfect matchings in G is
equal to the permanent of matrix A.

Let $A ∈ Cn×n$ . If there exists $X ∈ Cn×n$ satisfying the equations $AXA = A, XAX
= X$ and $AX = XA$, then such an X can be shown to be unique. This unique
solution is called the group inverse of A and is denoted by A`#` . The
nomenclature group inverse arises from the fact that the positive powers of A
and A`#` together with the idempotent matrix AA`#` as the identity element, form an
abelian group under matrix multiplication and was thus named by I.Erdelyi in
1967. The group inverse of a matrix $A ∈ Cn×n$ need not always exist. It is well
known that A`#` exists if and only if $R(A) ∩ N(A) = {0}$. In other words, A`#`
exists if and only if the index of A equals 1. Let us recall that the index of
a square matrix A is the smallest positive integer k such that rank(Ak ) =
rank(Ak+1 ).  If A is invertible, then the index is defined to be zero. An easy
dimensionality argument can be used to show that the index exists for any
square matrix. Thus, A`#` exists if and only if rank(A) = rank(A2 ).

linear matrix pencil:

$\lambda \in \mathbb{C} \text{or} \mathbb{R}$, $A,B$ are complex or real $n\times n$
matrices. $A,B \not= 0$

$A-\lambda B$

### $2\times 2$

$$
\operatorname{det}\begin{pmatrix}
a_1&b_1 ²nn¹ 
a_2&b_2 ²nn¹ 
\end{pmatrix}
=a_1b_2-b_1a_2
$$

For a $2\times 2$ matrix $A$ the characteristic
polynomial is $t^2-\operatorname{tr}(A)t+\operatorname{det}(A)=0$,
roots are the eigenvalues of $A$.

### Determinante

Construction:

(1) $\det(I)=1$

(2) Exchange two rows (to get a different matrix) reverses the sign of the determinant.

(3a) Multiply one of the rows by $t: \text{det} \Rightarrow t \cdot \text{det}$.

(3b) For any row, such that if it is the only row that changes:

$\displaystyle
\begin{array}{|cccc|} ... & & &
²nn¹ a+b' & b+b' & ... & n+n'
²nn¹ ... & & &
²nn¹ \end{array}
$

=

$$
\begin{array}{|cccc|}
    ... & & & ²nn¹ 
    a & b & ... & n  ²nn¹ 
    ... & & & ²nn¹ 
\end{array}
+
\begin{array}{|cccc|}
    ... & & & ²nn¹ 
    a' & b' & ... & n'  ²nn¹ 
    ... & & & ²nn¹ 
\end{array}
$$

This determines the Determinant.

Two equal rows -> det=0

Subtract $l\times \text{row} i \text{from row} k$ -> the det doesn't change.

Row of zeros -> det=0

$$\det(U)=\begin{array}{|ccc|}d_1&&\star  ²nn¹  &\ddots & ²nn¹  0& & d_n \end{array}=\prod_{i=0}^n d_i$$

$\det(A)=0$ if and only if $A$ is singular.

$\det(AB)=(\det A)(\det B)$

$\det(A^T)=\det(A)$

The $i,j$ cofactor of $B_{n\times n}$ is the scalar $C_{ij}$
defined by $C_{ij}=(-1)^{i+j}M_{ij}$, where $M_{ij}$ is
the determinant of the $(n-1)\times (n-1)$ matrix that
results from deleting the $i$-th row and the $j$-th column
of $B$. Then the Laplace expansion is given by the following.
Fix any $i,j\in \{1,2,...,n\}$.
Then the determinant $|B|$ is given by:

$$
|B|=b_{i1}C_{i1}+...+b_{in}C_{in}
=b_{ij}C_{1j}+...+b_{nj}C_{nj}
$$ $$
=\sum_{j\text{'}=1}^n b_{ij\text{'}}C_{ij\text{'}}
=\sum_{i\text{'}=1}^n b_{i\text{'}j}C_{i\text{'}j}
$$

$A^{-1}=\frac{1}{\operatorname{det}A}C^T$,
$C$ cofactor matrix.

$$
C=
\begin{pmatrix}
C_{11}&\hdots&C_{1n} ²nn¹ 
\vdots&&\vdots ²nn¹ 
C_{n1}&\hdots&C_{nn}
\end{pmatrix}
$$

$\det(A)=$ Volume of a box "spaned" as a "paralelogram" (paralelobox) by the rows of $A$.

Cofactor of $a_{ij}$ =
$C_{ij} ± \text{det}($ $n-1$ matrix with row $i$ and col $j$ erased $)$

+ if $i+j$ even, - if $i+j$ odd

Cofactor formula along row $i$:
$\text{det}A= a_{i1}C_{i1} + a_{i2}C_{i2} + \dots + a_{in}C_{in}$

from $(0,1)$-$n\times n$ matrix to $(-1,1)$-$(n+1)\times(n+1)$ matrix:


$\displaystyle
\begin{pmatrix} 1 & 0 & 1
²nn¹ 1 & 1 & 0
²nn¹ 0 & 1 & 1
²nn¹ \end{pmatrix}
$

-double->
\[
\begin{pmatrix}
2 & 0 & 2 ²n¹
2 & 2 & 0 ²n¹
0 & 2 & 2 ²n¹
\end{pmatrix}
\]
-boarder->
\[
\begin{pmatrix}
1 & 1 & 1 & 1 ²n¹
0 & 2 & 0 & 2 ²n¹
0 & 2 & 2 & 0 ²n¹
0 & 0 & 2 & 2 ²n¹
\end{pmatrix}
\]
-substract first row->
\[
\begin{pmatrix}
1 & 1 & 1 & 1 ²n¹
-1 & 1 & -1 & 1 ²n¹
-1 & 1 & 1 & -1 ²n¹
-1 & -1 & 1 & 1 ²n¹
\end{pmatrix}
\]

## Eigenvalues

$A_{n\times n}$ Matrix

$\lambda \in \mathbf{R}$

$x$ Eigenvector

$Ax=\lambda x$

$(A-\lambda I)x=0$

$\det (A-\lambda I)=0$

eigenvalues
$\lambda_i=\lambda_1,...,\lambda_n$

$\operatorname{trace}(A)=\sum_{i=1}^{n}\lambda_i$

$\det(A)=\prod_{i=1}^{n}\lambda_i$

$A^k$ hat $\lambda_i^k$

the eigenvalues of $A^{-1}$ are $\frac{1}{\lambda}$,
$\lambda$ eigenvalues of $A$.

## Jordon normal form

Any $n\times n$ matrix $A$ whose elements
are in an algebraically closed field $K$ is
similar to a **Jordon matrix** $J$, also in
$M_n(K)$, which is unique up to a permutation
of its diagonal blocks themselves. $J$ is
called the **Jordon normal form** of $A$.

$$J_i=\begin{pmatrix}
\lambda_i& 1& 0&\cdots&0 ²nn¹ 
0&\ddots&\ddots&\ddots&\vdots ²nn¹ 
\vdots&\ddots&\ddots&\ddots&0 ²nn¹ 
\vdots&&\ddots&\ddots&1 ²nn¹ 
0&\cdots&\cdots&0&\lambda_i
\end{pmatrix}
$$
$$
J=\begin{pmatrix}
J_1&&0 ²nn¹ 
&\ddots& ²nn¹ 
0&&J_d ²nn¹ 
\end{pmatrix}
$$

## Schur Decomposition

The Schur decomposition reads as follows: if A is a n ×
n square matrix with complex entries, then A can be expressed as

$$A = Q U Q^{-1}$$

where $Q$ is a unitary matrix (so that its inverse $Q^{−1}$ is
also the conjugate transpose $Q^\star $ of $Q$), and $U$ is an upper
triangular matrix, which is called a Schur form of $A$. Since $U$
is similar to $A$, it has the same multiset of eigenvalues, and
since it is triangular, those eigenvalues are the diagonal entries of $U$.

Given square matrices $A$ and $B$, the **generalized Schur decomposition**
factorizes both matrices as $A=QSZ^\star $ and $B=QTZ^\star $, where $Q$ and $Z$ are
unitary, and $S$ and $T$ are upper triangular. The generalized Schur
decomposition is also sometimes called the QZ decomposition.

The generalized eigenvalues $\lambda$ that solve the generalized eigenvalue problem $Ax=\lambda Bx$
(where $x$ is an unknown nonzero vector) can be calculated as the
ratio of the diagonal elements of $S$ to those of $T$. That
is, using subscripts to denote matrix elements, the ith generalized eigenvalue $\lambda_i$
satisfies $\lambda_i=S_{ii}/T_{ii}$.

### Redheffer matrix

$$R_{12\times 12}=\left({\begin{smallmatrix}
      1&1&1&1&1&1&1&1&1&1&1&1 ²nn¹ 
      1&1&0&1&0&1&0&1&0&1&0&1 ²nn¹ 
    1&0&1&0&0&1&0&0&1&0&0&1 ²nn¹ 
    1&0&0&1&0&0&0&1&0&0&0&1 ²nn¹ 
    1&0&0&0&1&0&0&0&0&1&0&0  ²nn¹ 
    1&0&0&0&0&1&0&0&0&0&0&1 ²nn¹ 1&0&0&0&0&0&1&0&0&0&0&0 ²nn¹ 
    1&0&0&0&0&0&0&1&0&0&0&0 ²nn¹ 1&0&0&0&0&0&0&0&1&0&0&0 ²nn¹ 
    1&0&0&0&0&0&0&0&0&1&0&0 ²nn¹ 1&0&0&0&0&0&0&0&0&0&1&0  ²nn¹ 
    1&0&0&0&0&0&0&0&0&0&0&1
\end{smallmatrix}}
    \right)$$

$\det(R_n)=M(n)$

$M(n)$, Mertens function, is the count of squarefree integers up to
$n$ that have an even number of prime factors, minus the count
of those that have an odd number.

$M(1)=1$, $M(2)=0$, $M(3)=-1$  
$M(10)=1$

$$M(n) = \sum_{k=1}^n \mu(k)$$ ($\mu$ Möbius function)

**Inner product**. $\langle ·,·\rangle :V\times V\to F$, $V$ vector space,
$F=\mathbb{R}$ or $\mathbb{C}$.  
$\langle x,y\rangle =\langle y,x\rangle$  
$\langle ax,y\rangle =a\langle x,y\rangle$  
$\langle x+y,z\rangle =\langle x,z\rangle \langle y,z\rangle$  
$\langle x,x\rangle \geqslant 0$  
$\langle x,x\rangle =0\Rightarrow x=0$ 

An inner product space is a vector space with an additional structure called an
inner product. (Skalarprodukt)

An inner product naturally induces an associated norm.

Sei $K=\mathbb{R}$ und $V$ ein euklidischer Vektorraum,
$v,w\ne 0$.
$$\langle v,w\rangle =\|v\|\|w\|\cos\varphi =v∙w=v^Tw$$

## LU decomposition

$$
U=
\begin{pmatrix}
d_1&&\star  ²nn¹ 
&\ddots& ²nn¹ 
0&&d_n
\end{pmatrix}
$$
$$
\operatorname{det}U=\prod d_i
$$

$A=LU$
$$\begin{pmatrix}
1&&0 ²nn¹ 
&1& ²nn¹ 
\star &&1
\end{pmatrix}
\begin{pmatrix}
\star &&\star  ²nn¹ 
&\star & ²nn¹ 
0&&\star
\end{pmatrix}
$$

$PA=LU$, $P$ permutation matrix, reorders the rows.
All square matrizes can be factorized in this form.

$$
\begin{pmatrix}
4&3 ²nn¹ 
6&3
\end{pmatrix} =
\begin{pmatrix}
1&0 ²nn¹ 
1.5&1
\end{pmatrix}
\begin{pmatrix}
4&3 ²nn¹ 
0&-1.5
\end{pmatrix}
$$

Row operations to get $U$.
Start with the identity matrix, the (same)
reverse row operations to get $L$.

$A=P^{-1}LU$  
$\operatorname{det}(A)=(-1)^s(\prod_{i=1}^n l_{ii})(\prod_{i=1}^n u_{ii})$,
$s$ number of row exchanges in the decomposition.

\minisec{system of linear equations}
$Ax=b$

$PA=LU$ or $A=LU$

$LUx=Pb$

1) solve $Ly=Pb$ for $y$.

2) solve $Ux=y$ for $x$.

\minisec{inverting a matrix}
$$
\begin{pmatrix}
a&b&1&0 ²nn¹ 
c&d&0&1
\end{pmatrix}
$$
row operations
$$
\begin{pmatrix}
1&0&a\text{'}&b\text{'} ²nn¹ 
0&1&c\text{'}&d\text{'}
\end{pmatrix}
$$

$AX=LUX=I$.
Solve for every column of $X$.

## Orthonormal basis

$$
u_j=\frac{w_i^{(j)}}{\|w_j^{(j)}\|},
w_k^{(j+1)}=w_k^{(j)}-w_k^{(j)}∙u_ju_j,
$$
$$
j=1,...,n,
$$
$$
k=j+1,...,n
\qquad;
$$
$$
w_1^{(1)}=w_1=\begin{pmatrix}2 ²nn¹ 2 ²nn¹ -1\end{pmatrix},
$$
$$
w_2^{(1)}=w_2=\begin{pmatrix}0 ²nn¹ 4 ²nn¹ -1\end{pmatrix},
$$
$$
w_3^{(1)}=w_2=\begin{pmatrix}1 ²nn¹ 2 ²nn¹ -3\end{pmatrix}
\qquad;
$$
$$
u_1=\frac{w_1^{(1)}}{\|w_1^{(1)}\|}=
\begin{pmatrix}\frac{2}{3} ²nn¹ \frac{2}{3} ²nn¹ -\frac{1}{3}\end{pmatrix},
$$
$$
w_2^{(2)}=w_2^{(1)}-w_2^{(1)}∙u_1u_1=\begin{pmatrix}-2 ²nn¹ 2 ²nn¹ 0\end{pmatrix},
$$
$$
w_3^{(2)}=w_3^{(1)}-w_3^{(1)}∙u_1u_1=\begin{pmatrix}-1 ²nn¹ 0 ²nn¹ -2\end{pmatrix}
\qquad;
$$
$$
u_2=\frac{w_2^{(2)}}{\|w_2^{(2)}\|}=\begin{pmatrix}-\frac{1}{\sqrt{2}} ²nn¹ 
\frac{1}{\sqrt{2}} ²nn¹ 0\end{pmatrix},
$$
$$
w_3^{(3)}=w_3^{(2)}-w_3^{(2)}∙u_2u_2=
\begin{pmatrix}-\frac{1}{2} ²nn¹ -\frac{1}{2} ²nn¹ -2\end{pmatrix}
\qquad;
$$
$$
u_3=\frac{w_3^{(3)}}{\|w_3^{(3)}\|}=
\begin{pmatrix}
-\frac{\sqrt{2}}{6} ²nn¹ -\frac{\sqrt{2}}{6} ²nn¹ -\frac{2\sqrt{2}}{3}
\end{pmatrix}
$$

## QR decomposition

the factorization is unique

## pseudoinverse

When referring to a matrix, the term pseudoinverse, without further specification, is
often used to indicate the Moore–Penrose pseudoinverse. 

For $A \in \mathrm{M}(m,n;K)$, a pseudoinverse of A is
defined as a matrix $A^+ \in \mathrm{M}(n, m; K)$ satisfying
all of the following four criteria:

$A A^+A = A$ ($AA^+$ need not be the general identity matrix, but it maps all
column vectors of A to themselves);

$A^+A A^+ = A^+$ ($A^+$ is a weak inverse for the multiplicative semigroup);

$(AA^+)^\star = AA^+$ ($AA^+$ is Hermitian); and

$(A^+A)^\star = A^+A$ ($A^+A$ is also Hermitian).

$A^+$ exists for any matrix, A , but when the latter has full rank, $A^+$ can
be expressed as a simple algebraic formula.

In particular, when A has linearly independent columns (and
thus matrix $A^\star A$ is invertible), $A^+$ can be computed
as:

$A^+ = (A^\star A)^{-1} A^\star $

This particular pseudoinverse constitutes a left inverse, since, in this case,
$A^+A = I$.

When A has linearly independent rows (matrix A $A^\star $ is invertible), $A^+$
can be computed as:

$A^+ = A^\star (A A^\star )^{-1}$

This is a right inverse, as $A A^+=I$.

The pseudoinverse is defined and unique for all
matrices whose entries are real or complex.
If $A=U\Sigma V^\star $ then $A^+=V\Sigma^+U^\star $,
$\Sigma^+$ reciprocal of non-zero elements
on the diagonal of $\Sigma$, and then transposing
the matrix. $Ax=b$ → $z=A^+b$.

If $A$ is invertible, its pseudoinverse is its inverse. That is: $A^+=A^{-1}$.

The pseudoinverse of the pseudoinverse is the original matrix: $(A^+)^+=A$.

Pseudoinversion commutes with transposition, conjugation, and taking the
conjugate transpose:

$(A^\mathrm{T})^+ = (A^+)^\mathrm{T},~~ (\,\overline{A}\,)^+ = \overline{A^+},~~ (A^\star )^+ = (A^+)^\star $

The pseudoinverse of a scalar multiple of A is the reciprocal multiple of A+:

$(\alpha A)^+ = \alpha^{-1} A^+\,\! for \alpha\neq 0$


## condition Number

A matrix that is not invertible has condition
number equal to infinity.

The condition number of a function with respect
to an argument measures how much the output value
of the function can change for a small change in
the input argument.

If $||\cdot||_2$ is the norm,

$$κ(A)=\frac{σ_{\max}(A)}{σ_{\min}(A)}$$

If $A$ is unitary then $κ(A)=1$.

If $A$ is normal then $κ(A)=|\frac{\lambda_{\max}(A)}{\lambda_{\min}(A)}|$

# Linear System of Equations

$$
\begin{matrix}
a_{11}x_1+\ldots+a_{1n}x_n = 0 ²nn¹ 
\vdots ²nn¹ 
a_{m1}x_1+\ldots+a_{mn}x_n = 0
\end{matrix}
$$
$\Rightarrow \mathbf{Ax}=\mathbf{0}$

$\mathbf{A}=\mathbf{U} \Sigma \mathbf{V}^\star $,
the vector $\mathbf{x}$ can be characterized
as a right-singular vector corresponding to a
singular value of $\mathbf{A}$ that is zero.

The presence of a zero singular value indicates
that the matrix is singular.

The number of non-zero singular values
indicates the rank of the matrix.

